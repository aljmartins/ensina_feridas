import os
import base64
from pathlib import Path
import io

import json
import re

import streamlit as st
import google.generativeai as genai

# PDF (ReportLab) ‚Äî exportar A4 com banner, rodap√© e numera√ß√£o
try:
    from reportlab.lib.pagesizes import A4
    from reportlab.lib.units import cm
    from reportlab.pdfgen import canvas
    from reportlab.lib.utils import ImageReader
    _PDF_OK = True
except Exception:
    _PDF_OK = False


# =========================
# Configura√ß√£o do app
# =========================
st.set_page_config(page_title="Ensina Feridas (Gemini)", layout="centered")

# Enxuga o topo do Streamlit (remove espa√ßo antes do banner)
st.markdown(
    """
    <style>
      header {visibility: hidden;}
      [data-testid="stToolbar"] {visibility: hidden;}
      [data-testid="stHeader"] {display:none;}
      [data-testid="stDecoration"] {display:none;}
      #MainMenu {visibility: hidden;}
      footer {visibility: hidden;}

      .block-container {
        padding-top: 0.25rem !important;
        padding-bottom: 1.2rem !important;
      }
      h1 { font-size: 1.6rem !important; margin-top: 0.2rem !important; }
    </style>
    """,
    unsafe_allow_html=True,
)

# Banner da p√°gina (UI)
banner_path = Path(__file__).parent / "assets" / "banner.pdf.a4.png"
if banner_path.exists():
    st.image(str(banner_path), use_container_width=True)
else:
    st.caption(f"Banner n√£o encontrado: {banner_path}")

st.markdown(
    "<h2 style='text-align:center; margin:0.25rem 0 0.25rem 0;'>ü©π Ensina Feridas </h2>",
    unsafe_allow_html=True,
)
st.caption("Streamlit + Gemini (SDK est√°vel: `google-generativeai`).")

from pathlib import Path
import base64
import streamlit as st

# --- Instagram icon ---
insta_path = Path("assets/instagram.png")
insta_b64 = base64.b64encode(insta_path.read_bytes()).decode()
enf_path = Path("assets/logo.enfermagem.png")
enf_b64 = base64.b64encode(enf_path.read_bytes()).decode()

st.markdown(
    f"""
    <div style="display:flex; align-items:center; gap:12px; margin-top:-10px; margin-bottom:12px;">
        <img src="data:image/png;base64,{insta_b64}" width="24">
        <a href="https://www.instagram.com/amorapele_ufpel/" target="_blank"
           style="text-decoration:none; font-weight:500;">
           Amor √† Pele
        </a>
        <span>|</span>
        <a href="https://www.instagram.com/g10petsaude/" target="_blank"
           style="text-decoration:none; font-weight:500;">
           PET G10
        </a>
        <span>|</span>
        <img src="data:image/png;base64,{enf_b64}" width="24">
        <a href="https://wp.ufpel.edu.br/fen/" target="_blank"
           style="text-decoration:none; font-weight:500;">
           Faculdade de Enfermagem ‚Äì UFPel
        </a>
    </div>
    """,
    unsafe_allow_html=True
)




# =========================
# Prompts do "GEM"
# =========================
CLINICAL_HINT = (
    "Voc√™ √© um especialista em feridas cr√¥nicas e protocolos de cuidado (TIME/TIMERS). "
    "Responda com orienta√ß√£o cl√≠nica segura e pr√°tica. "
    "Se faltarem dados, fa√ßa perguntas objetivas. "
    "Evite prescrever doses/condutas de alto risco sem contexto cl√≠nico. "
    "Quando houver sinais de alarme (ex.: infec√ß√£o sist√™mica, isquemia grave, dor desproporcional), "
    "recomende avalia√ß√£o presencial."
)

EDU_HINT = (
    "Voc√™ √© um especialista em ensino & aprendizagem no ensino superior (tutor). "
    "Seu objetivo √© ensinar, n√£o s√≥ responder. "
    "Use explica√ß√£o progressiva (do b√°sico ao avan√ßado), exemplos, analogias e perguntas diagn√≥sticas. "
    "Aplique metodologias ativas (PBL): formule hip√≥teses, pe√ßa dados que faltam e estimule racioc√≠nio. "
    "Sempre que poss√≠vel, devolva um mini-roteiro de estudo + um exerc√≠cio curto com gabarito comentado. "
    "Mantenha o foco em feridas cr√¥nicas e protocolos TIME/TIMERS, com seguran√ßa cl√≠nica."
)

mode = st.radio("Modo", ["Ensino (tutor)", "Cl√≠nico (objetivo)"], horizontal=True)
SYSTEM_HINT = EDU_HINT if mode == "Ensino (tutor)" else CLINICAL_HINT


# =========================
# API key (secrets > env)
# =========================
def get_api_key() -> str | None:
    for k in ("GOOGLE_API_KEY", "GEMINI_API_KEY"):
        try:
            v = st.secrets.get(k)
        except Exception:
            v = None
        if v:
            return str(v).strip()
    return (os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY") or "").strip() or None


api_key = get_api_key()
if not api_key:
    st.error("Faltou a chave da API. Defina GOOGLE_API_KEY ou GEMINI_API_KEY.")
    st.info(
        "No Streamlit Cloud: Settings ‚Üí Secrets\n\n"
        'GOOGLE_API_KEY = "SUA_CHAVE_AQUI"\n'
    )
    st.stop()

genai.configure(api_key=api_key)


# =========================
# Modelos dispon√≠veis (cache)
# =========================
@st.cache_data(ttl=3600, show_spinner=False)
def list_generate_models(_api_key_fingerprint: str) -> list[str]:
    models: list[str] = []
    try:
        for m in genai.list_models():
            methods = getattr(m, "supported_generation_methods", None) or []
            if "generateContent" in methods and getattr(m, "name", None):
                models.append(m.name)
    except Exception:
        models = []
    return models or [
        "models/gemini-2.0-flash",
        "models/gemini-2.0-flash-lite",
        "models/gemini-2.0-pro",
        "models/gemini-1.5-flash",
        "models/gemini-1.5-pro",
    ]


available_models = list_generate_models(api_key[:6] + "‚Ä¶" + api_key[-4:])

# =========================
# Controles do usu√°rio
# =========================
col1, col2 = st.columns([2, 1])

with col1:
    model_name = st.selectbox("Modelo", available_models, index=0)

with col2:
    temperature = st.slider(
        "Estilo da resposta",
        0.0,
        1.0,
        0.25 if mode == "Ensino (tutor)" else 0.3,
        0.05,
    )
    st.caption("Mais baixo = respostas objetivas ‚Ä¢ Mais alto = respostas mais explicativas")

auto_sketch = st.checkbox(
    "Sugerir esbo√ßo quando fizer sentido",
    value=True,
    help="O app decide automaticamente se um esbo√ßo/figura ajudaria na resposta e, se sim, gera um prompt pronto para voc√™ colar em um gerador de imagens.",
)

import streamlit.components.v1 as components

# =========================
# Caixa de pergunta com microfone
# =========================

# Microfone: injeta texto direto no textarea do Streamlit via DOM
components.html(
    """
    <style>
      .mic-row { display:flex; align-items:center; gap:10px; margin-bottom:2px; }
      .mic-btn {
        background:#0066cc; color:white; border:none;
        width:40px; height:40px; border-radius:50%;
        font-size:1.2rem; cursor:pointer; flex-shrink:0;
        display:flex; align-items:center; justify-content:center;
        transition:background .2s;
      }
      .mic-btn.listening { background:#e53935; animation:pulse 1s infinite; }
      @keyframes pulse {
        0%,100%{box-shadow:0 0 0 0 rgba(229,57,53,.35);}
        50%{box-shadow:0 0 0 8px rgba(229,57,53,0);}
      }
      #micStatus { font-size:.82rem; color:#555; }
    </style>
    <div class="mic-row">
      <button class="mic-btn" id="micBtn" title="Falar pergunta">üé§</button>
      <span id="micStatus">Toque em üé§ para falar (Chrome/Edge) ‚Äî o texto vai para a caixa abaixo</span>
    </div>
    <script>
    (function(){
      const btn    = document.getElementById('micBtn');
      const status = document.getElementById('micStatus');
      let listening = false;

      // Injeta texto no textarea do Streamlit que est√° na janela pai
      function fillStreamlitTextarea(text) {
        try {
          const doc = window.parent.document;
          // Streamlit renderiza textareas com data-testid="stTextArea" > textarea
          const ta = doc.querySelector('textarea[aria-label="Pergunta / caso"]')
                  || doc.querySelector('[data-testid="stTextArea"] textarea')
                  || doc.querySelector('textarea');
          if (!ta) { status.textContent = '‚ö†Ô∏è Caixa n√£o encontrada ‚Äî tente novamente.'; return; }
          // For√ßa React a reconhecer a mudan√ßa
          const nativeInputValueSetter = Object.getOwnPropertyDescriptor(window.parent.HTMLTextAreaElement.prototype, 'value').set;
          nativeInputValueSetter.call(ta, text);
          ta.dispatchEvent(new Event('input', { bubbles: true }));
          ta.dispatchEvent(new Event('change', { bubbles: true }));
          ta.focus();
          status.textContent = '‚úÖ Texto na caixa ‚Äî edite se quiser e clique Enviar.';
        } catch(e) {
          status.textContent = '‚ö†Ô∏è Erro ao preencher: ' + e.message;
        }
      }

      const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SR) {
        status.textContent = '‚ö†Ô∏è Voz indispon√≠vel ‚Äî use o üé§ do teclado do celular.';
        btn.style.opacity = '.4';
        btn.addEventListener('click', () => alert('Use Chrome ou Edge para reconhecimento de voz.'));
        return;
      }

      const rec = new SR();
      rec.lang = 'pt-BR';
      rec.continuous = false;
      rec.interimResults = true;

      rec.onstart = () => {
        listening = true;
        btn.classList.add('listening');
        btn.innerHTML = 'üî¥';
        status.textContent = 'üéôÔ∏è Ouvindo‚Ä¶';
      };
      rec.onresult = (e) => {
        let interim = '', final = '';
        for (let i = e.resultIndex; i < e.results.length; i++) {
          const t = e.results[i][0].transcript;
          if (e.results[i].isFinal) final += t; else interim += t;
        }
        if (final) { fillStreamlitTextarea(final.trim()); }
        else { status.textContent = '‚Ä¶' + interim; }
      };
      rec.onerror = (e) => {
        status.textContent = '‚ùå Erro: ' + e.error;
        btn.classList.remove('listening'); btn.innerHTML = 'üé§'; listening = false;
      };
      rec.onend = () => {
        listening = false; btn.classList.remove('listening'); btn.innerHTML = 'üé§';
      };
      btn.addEventListener('click', () => { if (listening) rec.stop(); else rec.start(); });
    })();
    </script>
    """,
    height=55,
    scrolling=False,
)

# Campo de texto nativo ‚Äî edit√°vel normalmente (voz ou digitado)
prompt = st.text_area(
    "Pergunta / caso",
    height=200,
    placeholder=(
        "Conte sua d√∫vida ou situa√ß√£o do dia a dia.\n"
        "Ex.: Tenho p√© diab√©tico e amanh√£ vou a um casamento. Que sapato posso usar?"
    ),
    key="prompt_area",
)

# Bot√£o nativo do Streamlit
enviar = st.button("üöÄ Enviar para o Gemini", type="primary")


def build_prompt(user_text: str) -> str:
    teaching_rules = ""
    if mode == "Ensino (tutor)":
        teaching_rules = (
            "\nFORMATO (modo ensino):"
            "\n1) Resposta curta (2‚Äì5 linhas) para situar."
            "\n2) Explica√ß√£o em passos (bullet points)."
            "\n3) Perguntas diagn√≥sticas (3‚Äì5)."
            "\n4) Exerc√≠cio r√°pido + gabarito comentado."
            "\n5) Alertas de seguran√ßa (se aplic√°vel).\n"
        )

    return f"""INSTRU√á√ïES (contexto):
{SYSTEM_HINT}

SOLICITA√á√ÉO DO USU√ÅRIO:
{user_text}

REGRAS GERAIS:
- Seja pr√°tico e did√°tico.
- Se houver risco (ex.: sinais de infec√ß√£o sist√™mica, isquemia grave, dor desproporcional), recomende avalia√ß√£o presencial.
{teaching_rules}
"""



# =========================
# Decis√£o: precisa de esbo√ßo?
# =========================
def decidir_esboco(pergunta: str, resposta: str, modelo_decisor: str | None = None) -> dict:
    """
    Usa o pr√≥prio Gemini para decidir se um esbo√ßo/figura ajuda, e prop√µe um prompt de imagem.
    Retorna dict com:
      - need_sketch: bool
      - reason: str
      - sketch_prompt: str
    """
    try:
        model_id = modelo_decisor or model_name
        model = genai.GenerativeModel(model_name=model_id)

        decisor_prompt = f"""
Voc√™ √© um assistente que decide se um ESBO√áO/FIGURA simples ajudaria a resposta.
Contexto: o app √© sobre feridas cr√¥nicas (TIME/TIMERS), mas a pergunta pode ser geral.

Responda SOMENTE em JSON v√°lido, SEM markdown, SEM texto extra, no formato:
{{"need_sketch": true/false, "reason": "...", "sketch_prompt": "..."}}

Regras:
- need_sketch = true quando uma figura melhoraria MUITO a compreens√£o (ex.: anatomia, posicionamento, escolha de cal√ßado/√≥rtese, passo-a-passo de curativo, fluxogramas, compara√ß√£o visual, layout de equipamento).
- need_sketch = false quando for pura explica√ß√£o textual, listas simples, ou quando um desenho pode induzir erro cl√≠nico.
- Se need_sketch = false, deixe sketch_prompt como string vazia "".
- Se need_sketch = true, crie um prompt curto, bem espec√≠fico, para gerar uma imagem did√°tica, sem conte√∫do chocante. Evite sangue expl√≠cito.

PERGUNTA:
{pergunta}

RESPOSTA (resumo):
{resposta[:1200]}
"""
        resp = model.generate_content(
            decisor_prompt,
            generation_config=genai.types.GenerationConfig(temperature=0.1),
        )
        raw = (getattr(resp, "text", None) or "").strip()

        # tenta JSON direto; se vier ‚Äúcom sujeira‚Äù, extrai o primeiro {...}
        try:
            data = json.loads(raw)
        except Exception:
            m = re.search(r"\{.*\}", raw, flags=re.DOTALL)
            data = json.loads(m.group(0)) if m else {"need_sketch": False, "reason": "N√£o consegui interpretar a decis√£o.", "sketch_prompt": ""}

        return {
            "need_sketch": bool(data.get("need_sketch", False)),
            "reason": str(data.get("reason", "")).strip(),
            "sketch_prompt": str(data.get("sketch_prompt", "")).strip(),
        }
    except Exception:
        return {"need_sketch": False, "reason": "Falha ao decidir esbo√ßo.", "sketch_prompt": ""}

# =========================
# Exporta√ß√£o PDF
# =========================
def wrap_text(text: str, max_chars: int = 110) -> list[str]:
    """Quebra texto em linhas por comprimento aproximado (robusto e simples)."""
    lines: list[str] = []
    for para in (text or "").splitlines():
        if not para.strip():
            lines.append("")
            continue
        words = para.split()
        line = ""
        for w in words:
            test = (line + " " + w).strip()
            if len(test) <= max_chars:
                line = test
            else:
                if line:
                    lines.append(line)
                line = w
        if line:
            lines.append(line)
    return lines


class NumberedCanvas(canvas.Canvas):
    def __init__(self, *args, footer_text: str = "", **kwargs):
        super().__init__(*args, **kwargs)
        self._saved_page_states = []
        self._footer_text = footer_text

    def showPage(self):
        self._saved_page_states.append(dict(self.__dict__))
        self._startPage()

    def save(self):
        num_pages = len(self._saved_page_states)
        for i, state in enumerate(self._saved_page_states, start=1):
            self.__dict__.update(state)
            self._draw_footer(i, num_pages)
            super().showPage()
        super().save()

    def _draw_footer(self, page_num: int, total_pages: int):
        width, _ = self._pagesize
        margin = 2 * cm
        y = 1.2 * cm

        self.setFont("Helvetica", 9)
        self.drawString(margin, y, self._footer_text)
        self.drawRightString(width - margin, y, f"P√°gina {page_num} de {total_pages}")


def gerar_pdf_a4(pergunta: str, resposta: str) -> bytes:
    """Gera PDF A4 com banner no cabe√ßalho, rodap√© fixo e numera√ß√£o."""
    footer_text = "PET G10 UFPel - Telemonitoramento de Feridas Cr√¥nicas"

    buffer = io.BytesIO()
    c = NumberedCanvas(buffer, pagesize=A4, footer_text=footer_text)
    largura, altura = A4

    margem_esq = 2 * cm
    margem_dir = 2 * cm
    margem_inf = 2 * cm
    y = altura - 2 * cm

    # --- Banner no topo (proporcional, sem deformar) ---
    banner = Path(__file__).parent / "assets" / "banner.pdf.a4.png"
    largura_util = largura - margem_esq - margem_dir
    max_banner_h = 3.2 * cm  # ajuste fino: 2.8‚Äì3.6 cm

    if banner.exists():
        img = ImageReader(str(banner))
        iw, ih = img.getSize()

        # escala para caber na largura
        escala = largura_util / float(iw)
        w = largura_util
        h = ih * escala

        # se ficou alto demais, limita pela altura m√°xima (mant√©m propor√ß√£o)
        if h > max_banner_h:
            escala = max_banner_h / float(ih)
            h = max_banner_h
            w = iw * escala

        # centraliza horizontalmente se sobrou espa√ßo (quando limitou pela altura)
        x = margem_esq + (largura_util - w) / 2.0

        c.drawImage(
            img,
            x,
            y - h,
            width=w,
            height=h,
            preserveAspectRatio=True,
            mask="auto",
        )
        y -= h + 0.8 * cm
    else:
        c.setFont("Helvetica-Bold", 10)
        c.drawString(margem_esq, y, f"Banner n√£o encontrado: {banner}")
        y -= 0.8 * cm

    c.setFont("Helvetica", 10)
    for linha in wrap_text(pergunta):
        if y < (margem_inf + 1.6 * cm):  # reserva espa√ßo pro rodap√©
            c.showPage()
            y = altura - 2 * cm
            c.setFont("Helvetica", 10)
        c.drawString(margem_esq, y, linha)
        y -= 0.45 * cm

    y -= 0.8 * cm
    if y < (margem_inf + 1.6 * cm):
        c.showPage()
        y = altura - 2 * cm

    c.setFont("Helvetica-Bold", 12)
    c.drawString(margem_esq, y, "Resposta do Sistema")
    y -= 0.6 * cm

    c.setFont("Helvetica", 10)
    for linha in wrap_text(resposta):
        if y < (margem_inf + 1.6 * cm):
            c.showPage()
            y = altura - 2 * cm
            c.setFont("Helvetica", 10)
        c.drawString(margem_esq, y, linha)
        y -= 0.45 * cm

    c.save()
    buffer.seek(0)
    return buffer.getvalue()


# =========================
# Execu√ß√£o
# =========================
# O envio √© feito pelo bot√£o dentro do componente HTML acima
if enviar and prompt:
    if not prompt.strip():
        st.warning("Escreve algo antes. O modelo n√£o l√™ pensamento (ainda). üòÑ")
        st.stop()

    with st.spinner("Gerando resposta..."):
        try:
            model = genai.GenerativeModel(model_name=model_name)
            resp = model.generate_content(
                build_prompt(prompt),
                generation_config=genai.types.GenerationConfig(
                    temperature=temperature,
                ),
            )

            st.subheader("Resposta:")
            text = getattr(resp, "text", None)
            final_text = text if text else str(resp)
            st.write(final_text)

            # --- Sugest√£o de esbo√ßo (quando fizer sentido) ---
            if auto_sketch:
                with st.spinner("Checando se um esbo√ßo ajudaria‚Ä¶"):
                    d = decidir_esboco(prompt, final_text)
                if d.get("need_sketch"):
                    st.markdown("### ‚úçÔ∏è Sugest√£o de esbo√ßo")
                    if d.get("reason"):
                        st.info(d["reason"])

                    # B√îNUS ELEGANTE: mostra o prompt com wrap + evita vari√°vel n√£o definida
                    sketch = (d.get("sketch_prompt") or "").strip()
                    if sketch:
                        st.info("‚úÖ O sistema sugere que um esbo√ßo ajudaria. Copie o prompt abaixo.")
                        st.text_area(
                            "Prompt do esbo√ßo (PT-BR) ‚Äî copie e cole no gerador de imagens",
                            value=sketch,
                            height=200,
                        )
                        st.download_button(
                            "Baixar prompt do esbo√ßo (.txt)",
                            data=sketch.encode("utf-8"),
                            file_name="prompt_esboco.txt",
                            mime="text/plain; charset=utf-8",
                        )
                    else:
                        st.caption("O decisor marcou que um esbo√ßo ajudaria, mas n√£o gerou um prompt. Tente novamente ou reformule o caso.")

                    st.caption(
                        "Dica: cole esse prompt em um gerador de imagens (NanoBanana, Leonardo, Stable Diffusion, etc.). "
                        "Se quiser, eu tamb√©m posso adaptar o prompt para a ferramenta que voc√™ for usar."
                    )

            # Guarda para exporta√ß√£o em PDF
            st.session_state["ultima_pergunta"] = prompt
            st.session_state["ultima_resposta"] = final_text
            st.session_state["ultimo_modelo"] = model_name
            st.session_state["ultimo_modo"] = mode

            # Bot√£o de exporta√ß√£o (fica logo ap√≥s a resposta)
            st.markdown("### üìÑ Exportar resposta")
            if not _PDF_OK:
                st.warning("Exporta√ß√£o PDF indispon√≠vel: instale `reportlab` no requirements.txt.")
            else:
                pdf_bytes = gerar_pdf_a4(prompt, final_text)
                st.download_button(
                    "Gerar PDF A4 (com banner)",
                    data=pdf_bytes,
                    file_name="ensina_feridas_resposta.pdf",
                    mime="application/pdf",
                    key="download_pdf_a4",
                )

        except Exception as e:
            st.error("Erro ao chamar o Gemini:")
            st.exception(e)

st.divider()

